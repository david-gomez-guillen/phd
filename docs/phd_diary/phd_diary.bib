@inproceedings{proposal:hypermapper,
	author    = {Luigi Nardi and
	Artur L. F. Souza and
	David Koeplinger and
	Kunle Olukotun},
	title     = {HyperMapper: a Practical Design Space Exploration Framework},
	booktitle = {27th {IEEE} International Symposium on Modeling, Analysis, and Simulation
	of Computer and Telecommunication Systems, {MASCOTS} 2019, Rennes,
	France, October 21-25, 2019},
	pages     = {425--426},
	publisher = {{IEEE} Computer Society},
	year      = {2019},
	url       = {https://doi.org/10.1109/MASCOTS.2019.00053},
	doi       = {10.1109/MASCOTS.2019.00053},
	timestamp = {Thu, 09 Jul 2020 01:00:00 +0200},
	biburl    = {https://dblp.org/rec/conf/mascots/NardiSKO19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{proposal:fast_gaussian,
	author    = {Sourish Das and
	Sasanka Roy and
	Rajiv Sambasivan},
	title     = {Fast Gaussian Process Regression for Big Data},
	journal   = {CoRR},
	volume    = {abs/1509.05142},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.05142},
	eprinttype = {arXiv},
	eprint    = {1509.05142},
	timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/DasRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{proposal:splitting_gaussian,
	doi = {10.1371/journal.pone.0256470},
	author = {Terry, Nick AND Choe, Youngjun},
	journal = {PLOS ONE},
	publisher = {Public Library of Science},
	title = {Splitting Gaussian processes for computationally-efficient regression},
	year = {2021},
	month = {08},
	volume = {16},
	url = {https://doi.org/10.1371/journal.pone.0256470},
	pages = {1-17},
	abstract = {Gaussian processes offer a flexible kernel method for regression. While Gaussian processes have many useful theoretical properties and have proven practically useful, they suffer from poor scaling in the number of observations. In particular, the cubic time complexity of updating standard Gaussian process models can be a limiting factor in applications. We propose an algorithm for sequentially partitioning the input space and fitting a localized Gaussian process to each disjoint region. The algorithm is shown to have superior time and space complexity to existing methods, and its sequential nature allows the model to be updated efficiently. The algorithm constructs a model for which the time complexity of updating is tightly bounded above by a pre-specified parameter. To the best of our knowledge, the model is the first local Gaussian process regression model to achieve linear memory complexity. Theoretical continuity properties of the model are proven. We demonstrate the efficacy of the resulting model on several multi-dimensional regression tasks.},
	number = {8},
	
}

@misc{proposal:parallel_bayesian,
	doi = {10.48550/ARXIV.2105.08195},	
	url = {https://arxiv.org/abs/2105.08195},	
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement},	
	publisher = {arXiv},	
	year = {2021},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{proposal:parallel_bayesian2,
	doi = {10.48550/ARXIV.1602.05149},	
	url = {https://arxiv.org/abs/1602.05149},	
	author = {Wang, Jialei and Clark, Scott C. and Liu, Eric and Frazier, Peter I.},	
	keywords = {Machine Learning (stat.ML), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},	
	title = {Parallel Bayesian Global Optimization of Expensive Functions},	
	publisher = {arXiv},	
	year = {2016},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{proposal:bayesian_prior,
	doi = {10.48550/ARXIV.2006.14608},	
	url = {https://arxiv.org/abs/2006.14608},	
	author = {Souza, Artur and Nardi, Luigi and Oliveira, Leonardo B. and Olukotun, Kunle and Lindauer, Marius and Hutter, Frank},	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {Bayesian Optimization with a Prior for the Optimum},	
	publisher = {arXiv},	
	year = {2020},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{proposal:bayesian_constraints,
	doi = {10.48550/ARXIV.2105.13245},	
	url = {https://arxiv.org/abs/2105.13245},	
	author = {Ungredda, Juan and Branke, Juergen},	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {Bayesian Optimisation for Constrained Problems},	
	publisher = {arXiv},	
	year = {2021},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{proposal:bayesian_constraints2,
	title = 	 {Bayesian Optimization with Inequality Constraints},
	author = 	 {Gardner, Jacob and Kusner, Matt and Zhixiang  and Weinberger, Kilian and Cunningham, John},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {937--945},
	year = 	 {2014},
	editor = 	 {Xing, Eric P. and Jebara, Tony},
	volume = 	 {32},
	number =       {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/gardner14.pdf},
	url = 	 {https://proceedings.mlr.press/v32/gardner14.html},
	abstract = 	 {Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations.  It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design.  However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective.  Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions.  We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly find optimal and feasible points, even when small feasible regions cause standard methods to fail.}
}
