@inproceedings{proposal:hypermapper,
	author    = {Luigi Nardi and
	Artur L. F. Souza and
	David Koeplinger and
	Kunle Olukotun},
	title     = {HyperMapper: a Practical Design Space Exploration Framework},
	booktitle = {27th {IEEE} International Symposium on Modeling, Analysis, and Simulation
	of Computer and Telecommunication Systems, {MASCOTS} 2019, Rennes,
	France, October 21-25, 2019},
	pages     = {425--426},
	publisher = {{IEEE} Computer Society},
	year      = {2019},
	url       = {https://doi.org/10.1109/MASCOTS.2019.00053},
	doi       = {10.1109/MASCOTS.2019.00053},
	timestamp = {Thu, 09 Jul 2020 01:00:00 +0200},
	biburl    = {https://dblp.org/rec/conf/mascots/NardiSKO19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{proposal:fast_gaussian,
	author    = {Sourish Das and
	Sasanka Roy and
	Rajiv Sambasivan},
	title     = {Fast Gaussian Process Regression for Big Data},
	journal   = {CoRR},
	volume    = {abs/1509.05142},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.05142},
	eprinttype = {arXiv},
	eprint    = {1509.05142},
	timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/DasRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{proposal:splitting_gaussian,
	doi = {10.1371/journal.pone.0256470},
	author = {Terry, Nick AND Choe, Youngjun},
	journal = {PLOS ONE},
	publisher = {Public Library of Science},
	title = {Splitting Gaussian processes for computationally-efficient regression},
	year = {2021},
	month = {08},
	volume = {16},
	url = {https://doi.org/10.1371/journal.pone.0256470},
	pages = {1-17},
	abstract = {Gaussian processes offer a flexible kernel method for regression. While Gaussian processes have many useful theoretical properties and have proven practically useful, they suffer from poor scaling in the number of observations. In particular, the cubic time complexity of updating standard Gaussian process models can be a limiting factor in applications. We propose an algorithm for sequentially partitioning the input space and fitting a localized Gaussian process to each disjoint region. The algorithm is shown to have superior time and space complexity to existing methods, and its sequential nature allows the model to be updated efficiently. The algorithm constructs a model for which the time complexity of updating is tightly bounded above by a pre-specified parameter. To the best of our knowledge, the model is the first local Gaussian process regression model to achieve linear memory complexity. Theoretical continuity properties of the model are proven. We demonstrate the efficacy of the resulting model on several multi-dimensional regression tasks.},
	number = {8},
	
}

@misc{proposal:parallel_bayesian,
	doi = {10.48550/ARXIV.2105.08195},	
	url = {https://arxiv.org/abs/2105.08195},	
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},	
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement},	
	publisher = {arXiv},	
	year = {2021},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{proposal:parallel_bayesian2,
	doi = {10.48550/ARXIV.1602.05149},	
	url = {https://arxiv.org/abs/1602.05149},	
	author = {Wang, Jialei and Clark, Scott C. and Liu, Eric and Frazier, Peter I.},	
	keywords = {Machine Learning (stat.ML), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},	
	title = {Parallel Bayesian Global Optimization of Expensive Functions},	
	publisher = {arXiv},	
	year = {2016},	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{paulson_cobalt_2021,
	title = {{COBALT}: {COnstrained} Bayesian {optimizAtion} of {computationaLly} expensive grey-box models exploiting {derivaTive} information},
	url = {http://arxiv.org/abs/2105.04114},
	shorttitle = {{COBALT}},
	abstract = {Many engineering problems involve the optimization of computationally expensive models for which derivative information is not readily available. The Bayesian optimization ({BO}) framework is a particularly promising approach for solving these problems, which uses Gaussian process ({GP}) models and an expected utility function to systematically tradeoff between exploitation and exploration of the design space. {BO}, however, is fundamentally limited by the black-box model assumption that does not take into account any underlying problem structure. In this paper, we propose a new algorithm, {COBALT}, for constrained grey-box optimization problems that combines multivariate {GP} models with a novel constrained expected utility function whose structure can be exploited by state-of-the-art nonlinear programming solvers. {COBALT} is compared to traditional {BO} on seven test problems including the calibration of a genome-scale bioreactor model to experimental data. Overall, {COBALT} shows very promising performance on both unconstrained and constrained test problems.},
	journaltitle = {{arXiv}:2105.04114 [math]},
	author = {Paulson, Joel A. and Lu, Congwen},
	urldate = {2021-10-27},
	date = {2021-05-10},
	eprinttype = {arxiv},
	eprint = {2105.04114},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\2Q5GC4WB\\Paulson and Lu - 2021 - COBALT COnstrained Bayesian optimizAtion of compu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\BU95UTMF\\2105.html:text/html},
}

@article{gardner_bayesian_nodate,
	title = {Bayesian Optimization with Inequality Constraints},
	abstract = {Bayesian optimization is a powerful framework for minimizing expensive objective functions while using very few function evaluations. It has been successfully applied to a variety of problems, including hyperparameter tuning and experimental design. However, this framework has not been extended to the inequality-constrained optimization setting, particularly the setting in which evaluating feasibility is just as expensive as evaluating the objective. Here we present constrained Bayesian optimization, which places a prior distribution on both the objective and the constraint functions. We evaluate our method on simulated and real data, demonstrating that constrained Bayesian optimization can quickly ﬁnd optimal and feasible points, even when small feasible regions cause standard methods to fail.},
	pages = {9},
	author = {Gardner, Jacob R and Kusner, Matt J and Jake, Gardner},
	langid = {english},
	file = {Gardner et al. - Bayesian Optimization with Inequality Constraints.pdf:C\:\\Users\\David\\Zotero\\storage\\H9GSZUMS\\Gardner et al. - Bayesian Optimization with Inequality Constraints.pdf:application/pdf},
}

@article{ungredda_bayesian_2021,
	title = {Bayesian Optimisation for Constrained Problems},
	url = {http://arxiv.org/abs/2105.13245},
	abstract = {Many real-world optimisation problems such as hyperparameter tuning in machine learning or simulation-based optimisation can be formulated as expensive-to-evaluate black-box functions. A popular approach to tackle such problems is Bayesian optimisation ({BO}), which builds a response surface model based on the data collected so far, and uses the mean and uncertainty predicted by the model to decide what information to collect next. In this paper, we propose a novel variant of the well-known Knowledge Gradient acquisition function that allows it to handle constraints. We empirically compare the new algorithm with four other state-of-the-art constrained Bayesian optimisation algorithms and demonstrate its superior performance. We also prove theoretical convergence in the infinite budget limit.},
	journaltitle = {{arXiv}:2105.13245 [cs, stat]},
	author = {Ungredda, Juan and Branke, Juergen},
	urldate = {2021-10-27},
	date = {2021-05-27},
	eprinttype = {arxiv},
	eprint = {2105.13245},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\85XTYDSD\\Ungredda and Branke - 2021 - Bayesian Optimisation for Constrained Problems.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\KA2PLXIR\\2105.html:text/html},
}

@article{hernandez-lobato_general_2016,
	title = {A General Framework for Constrained Bayesian Optimization using Information-based Search},
	url = {http://arxiv.org/abs/1511.09422},
	abstract = {We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a {CPU} and the constraints are evaluated independently on a {GPU}. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints ({PESC}). {PESC} is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of {PESC} and in the actual evaluation of the target objective. We take a bounded rationality approach and develop partial update for {PESC} which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for {PESC}. This allows us to interpolate between versions of {PESC} that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that {PESC} is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.},
	journaltitle = {{arXiv}:1511.09422 [stat]},
	author = {Hernández-Lobato, José Miguel and Gelbart, Michael A. and Adams, Ryan P. and Hoffman, Matthew W. and Ghahramani, Zoubin},
	urldate = {2021-10-27},
	date = {2016-09-04},
	eprinttype = {arxiv},
	eprint = {1511.09422},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\93Q9FWYU\\Hernández-Lobato et al. - 2016 - A General Framework for Constrained Bayesian Optim.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\ZTTZX3Y7\\1511.html:text/html},
}

@article{swiler_survey_2020,
	title = {A Survey of Constrained Gaussian Process Regression: Approaches and Implementation Challenges},
	volume = {1},
	issn = {2689-3967},
	url = {http://arxiv.org/abs/2006.09319},
	doi = {10.1615/JMachLearnModelComput.2020035155},
	shorttitle = {A Survey of Constrained Gaussian Process Regression},
	abstract = {Gaussian process regression is a popular Bayesian framework for surrogate modeling of expensive data sources. As part of a broader eﬀort in scientiﬁc machine learning, many recent works have incorporated physical constraints or other a priori information within Gaussian process regression to supplement limited data and regularize the behavior of the model. We provide an overview and survey of several classes of Gaussian process constraints, including positivity or bound constraints, monotonicity and convexity constraints, diﬀerential equation constraints provided by linear {PDEs}, and boundary condition constraints. We compare the strategies behind each approach as well as the diﬀerences in implementation, concluding with a discussion of the computational challenges introduced by constraints.},
	pages = {119--156},
	number = {2},
	journaltitle = {Journal of Machine Learning for Modeling and Computing},
	shortjournal = {J Mach Learn Model Comput},
	author = {Swiler, Laura and Gulian, Mamikon and Frankel, Ari and Safta, Cosmin and Jakeman, John},
	urldate = {2022-08-03},
	date = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.09319 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {Swiler et al. - 2020 - A Survey of Constrained Gaussian Process Regressio.pdf:C\:\\Users\\David\\Zotero\\storage\\9SA8Q5PZ\\Swiler et al. - 2020 - A Survey of Constrained Gaussian Process Regressio.pdf:application/pdf},
}

@article{lam_lookahead_nodate,
	title = {Lookahead  Bayesian Optimization with Inequality Constraints},
	abstract = {We consider the task of optimizing an objective function subject to inequality constraints when both the objective and the constraints are expensive to evaluate. Bayesian optimization ({BO}) is a popular way to tackle optimization problems with expensive objective function evaluations, but has mostly been applied to unconstrained problems. Several {BO} approaches have been proposed to address expensive constraints but are limited to greedy strategies maximizing immediate reward. To address this limitation, we propose a lookahead approach that selects the next evaluation in order to maximize the long-term feasible reduction of the objective function. We present numerical experiments demonstrating the performance improvements of such a lookahead approach compared to several greedy {BO} algorithms, including constrained expected improvement ({EIC}) and predictive entropy search with constraint ({PESC}).},
	pages = {11},
	author = {Lam, Remi and Willcox, Karen},
	langid = {english},
	file = {Lam y Willcox - Lookahead  Bayesian Optimization with Inequality C.pdf:C\:\\Users\\David\\Zotero\\storage\\A7NYR3WY\\Lam y Willcox - Lookahead  Bayesian Optimization with Inequality C.pdf:application/pdf},
}

@article{duvenaud_automatic_nodate,
	title = {Automatic Model Construction  with Gaussian Processes},
	pages = {157},
	author = {Duvenaud, David Kristjanson},
	langid = {english},
	file = {Duvenaud - Automatic Model Construction  with Gaussian Proces.pdf:C\:\\Users\\David\\Zotero\\storage\\LJISYNPC\\Duvenaud - Automatic Model Construction  with Gaussian Proces.pdf:application/pdf},
}


@book{rasmussen_gaussian_2006,
	location = {Cambridge, Mass},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	series = {Adaptive computation and machine learning},
	pagetotal = {248},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	date = {2006},
	langid = {english},
	note = {{OCLC}: ocm61285753},
	keywords = {Machine learning, Data processing, Gaussian processes, Mathematical models},
	file = {Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf:C\:\\Users\\David\\Zotero\\storage\\CT6P6CGZ\\Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf:application/pdf},
}

@misc{li_gaussian_2022,
title = {Gaussian Process Bandit Optimization with Few Batches},
url = {http://arxiv.org/abs/2110.07788},
doi = {10.48550/arXiv.2110.07788},
abstract = {In this paper, we consider the problem of black-box optimization using Gaussian Process ({GP}) bandit optimization with a small number of batches. Assuming the unknown function has a low norm in the Reproducing Kernel Hilbert Space ({RKHS}), we introduce a batch algorithm inspired by batched finite-arm bandit algorithms, and show that it achieves the cumulative regret upper bound \$O{\textasciicircum}{\textbackslash}ast({\textbackslash}sqrt\{T{\textbackslash}gamma\_T\})\$ using \$O({\textbackslash}log{\textbackslash}log T)\$ batches within time horizon \$T\$, where the \$O{\textasciicircum}{\textbackslash}ast({\textbackslash}cdot)\$ notation hides dimension-independent logarithmic factors and \${\textbackslash}gamma\_T\$ is the maximum information gain associated with the kernel. This bound is near-optimal for several kernels of interest and improves on the typical \$O{\textasciicircum}{\textbackslash}ast({\textbackslash}sqrt\{T\}{\textbackslash}gamma\_T)\$ bound, and our approach is arguably the simplest among algorithms attaining this improvement. In addition, in the case of a constant number of batches (not depending on \$T\$), we propose a modified version of our algorithm, and characterize how the regret is impacted by the number of batches, focusing on the squared exponential and Mat{\textbackslash}'ern kernels. The algorithmic upper bounds are shown to be nearly minimax optimal via analogous algorithm-independent lower bounds.},
number = {{arXiv}:2110.07788},
publisher = {{arXiv}},
author = {Li, Zihan and Scarlett, Jonathan},
urldate = {2022-08-08},
date = {2022-02-21},
eprinttype = {arxiv},
eprint = {2110.07788 [cs, math, stat]},
keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\LPTCHLNK\\Li y Scarlett - 2022 - Gaussian Process Bandit Optimization with Few Batc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\RIBQLQA7\\2110.html:text/html},
}


@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for Hyper-Parameter Optimization},
	volume = {24},
	url = {https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bergstra, James and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	urldate = {2021-11-17},
	date = {2011},
	file = {Full Text PDF:C\:\\Users\\David\\Zotero\\storage\\T82TXDKA\\Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf:application/pdf},
}


@article{stark_constraints_2015,
	title = {Constraints versus Priors},
	volume = {3},
	issn = {2166-2525},
	url = {http://epubs.siam.org/doi/10.1137/130920721},
	doi = {10.1137/130920721},
	abstract = {There are deep and important philosophical diﬀerences between Bayesian and frequentist approaches to quantifying uncertainty. However, some practitioners choose between these approaches primarily on the basis of convenience. For instance, the ability to incorporate parameter constraints is sometimes cited as a reason to use Bayesian methods. This reﬂects two misunderstandings: First, frequentist methods can indeed incorporate constraints on parameter values. Second, it ignores the crucial question of what the result of the analysis will mean. Bayesian and frequentist measures of uncertainty have similar sounding names but quite diﬀerent meanings. For instance, Bayesian uncertainties typically involve expectations with respect to the posterior distribution of the parameter, holding the data ﬁxed; frequentist uncertainties typically involve expectations with respect to the distribution of the data, holding the parameter ﬁxed. Bayesian methods, including methods incorporating parameter constraints, require supplementing the constraints with a prior probability distribution for parameter values. This can cause frequentist and Bayesian estimates and their nominal uncertainties to diﬀer substantially, even when the prior is “uninformative.” This paper gives simple examples where “uninformative” priors are, in fact, extremely informative, and sketches how to measure how much information the prior adds to the constraint. Bayesian methods can have good frequentist behavior, and a frequentist can use Bayesian methods and quantify the uncertainty by frequentist means—but absent a meaningful prior, Bayesian uncertainty measures lack meaning. The paper ends with brief reﬂections on practice.},
	pages = {586--598},
	number = {1},
	journaltitle = {{SIAM}/{ASA} Journal on Uncertainty Quantification},
	shortjournal = {{SIAM}/{ASA} J. Uncertainty Quantification},
	author = {Stark, Philip B.},
	urldate = {2021-11-15},
	date = {2015-01},
	langid = {english},
	file = {Stark - 2015 - Constraints versus Priors.pdf:C\:\\Users\\David\\Zotero\\storage\\GVGY2RVR\\Stark - 2015 - Constraints versus Priors.pdf:application/pdf},
}

@article{souza_bayesian_2021,
	title = {Bayesian Optimization with a Prior for the Optimum},
	url = {http://arxiv.org/abs/2006.14608},
	abstract = {While Bayesian Optimization ({BO}) is a very popular method for optimizing expensive black-box functions, it fails to leverage the experience of domain experts. This causes {BO} to waste function evaluations on bad design choices (e.g., machine learning hyperparameters) that the expert already knows to work poorly. To address this issue, we introduce Bayesian Optimization with a Prior for the Optimum ({BOPrO}). {BOPrO} allows users to inject their knowledge into the optimization process in the form of priors about which parts of the input space will yield the best performance, rather than {BO}'s standard priors over functions, which are much less intuitive for users. {BOPrO} then combines these priors with {BO}'s standard probabilistic model to form a pseudo-posterior used to select which points to evaluate next. We show that {BOPrO} is around 6.67x faster than state-of-the-art methods on a common suite of benchmarks, and achieves a new state-of-the-art performance on a real-world hardware design application. We also show that {BOPrO} converges faster even if the priors for the optimum are not entirely accurate and that it robustly recovers from misleading priors.},
	journaltitle = {{arXiv}:2006.14608 [cs, stat]},
	author = {Souza, Artur and Nardi, Luigi and Oliveira, Leonardo B. and Olukotun, Kunle and Lindauer, Marius and Hutter, Frank},
	urldate = {2021-10-27},
	date = {2021-04-19},
	eprinttype = {arxiv},
	eprint = {2006.14608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\RNIQZRHN\\Souza et al. - 2021 - Bayesian Optimization with a Prior for the Optimum.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\3PDK6Q4Z\\2006.html:text/html},
}

@article{repicky_automated_nodate,
	title = {Automated Selection of Covariance Function for Gaussian Process Surrogate Models},
	abstract = {Gaussian processes have a long tradition in model-based algorithms for black-box optimization, where a limited number of objective function evaluations are available. A principal choice in specifying a Gaussian process model is the choice of the covariance function, which largely embodies the prior assumptions about the modeled function. Several methods for learning the form of covariance function have been proposed. We report a work in progress in which the covariance function is selected from a ﬁxed set. The goal of covariance function selection is to capture non-local properties of the objective function and derive a more accurate surrogate model. The model-selection algorithm is evaluated in connection with Doubly Trained Surrogate Covariance Matrix Adaptation Evolution Strategy on the Comparing Continuous Optimizers framework. Several estimates of predictive performance, including cross-validation and information criteria, are discussed. Focus is placed on information criteria suitable for nonparametric methods, and two of them are compared experimentally.},
	pages = {8},
	author = {Repický, Jakub and Pitra, Zbyneˇk and Holenˇa, Martin},
	langid = {english},
	file = {Repický et al. - Automated Selection of Covariance Function for Gau.pdf:C\:\\Users\\David\\Zotero\\storage\\75X8C7UK\\Repický et al. - Automated Selection of Covariance Function for Gau.pdf:application/pdf},
}

@article{duvenaud_additive_2011,
	title = {Additive Gaussian Processes},
	url = {http://arxiv.org/abs/1112.4394},
	abstract = {We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive {GPs} generalize both Generalized Additive Models, and the standard {GP} models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning ({HKL}). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.},
	journaltitle = {{arXiv}:1112.4394 [cs, stat]},
	author = {Duvenaud, David and Nickisch, Hannes and Rasmussen, Carl Edward},
	urldate = {2022-02-25},
	date = {2011-12-19},
	eprinttype = {arxiv},
	eprint = {1112.4394},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\4JA326NN\\Duvenaud et al. - 2011 - Additive Gaussian Processes.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\UTZ8GWUM\\1112.html:text/html},
}


@misc{nguyen_budgeted_2017,
	title = {Budgeted Batch Bayesian Optimization With Unknown Batch Sizes},
	url = {http://arxiv.org/abs/1703.04842},
	doi = {10.48550/arXiv.1703.04842},
	abstract = {Parameter settings profoundly impact the performance of machine learning algorithms and laboratory experiments. The classical grid search or trial-error methods are exponentially expensive in large parameter spaces, and Bayesian optimization ({BO}) offers an elegant alternative for global optimization of black box functions. In situations where the black box function can be evaluated at multiple points simultaneously, batch Bayesian optimization is used. Current batch {BO} approaches are restrictive in that they fix the number of evaluations per batch, and this can be wasteful when the number of specified evaluations is larger than the number of real maxima in the underlying acquisition function. We present the Budgeted Batch Bayesian Optimization (B3O) for hyper-parameter tuning and experimental design - we identify the appropriate batch size for each iteration in an elegant way. To set the batch size flexible, we use the infinite Gaussian mixture model ({IGMM}) for automatically identifying the number of peaks in the underlying acquisition functions. We solve the intractability of estimating the {IGMM} directly from the acquisition function by formulating the batch generalized slice sampling to efficiently draw samples from the acquisition function. We perform extensive experiments for both synthetic functions and two real world applications - machine learning hyper-parameter tuning and experimental design for alloy hardening. We show empirically that the proposed B3O outperforms the existing fixed batch {BO} approaches in finding the optimum whilst requiring a fewer number of evaluations, thus saving cost and time.},
	number = {{arXiv}:1703.04842},
	publisher = {{arXiv}},
	author = {Nguyen, Vu and Rana, Santu and Gupta, Sunil and Li, Cheng and Venkatesh, Svetha},
	urldate = {2022-08-08},
	date = {2017-04-15},
	eprinttype = {arxiv},
	eprint = {1703.04842 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\4YMCAZU9\\Nguyen et al. - 2017 - Budgeted Batch Bayesian Optimization With Unknown .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\NG252QYX\\1703.html:text/html},
}


@article{gonzalez_batch_nodate,
	title = {Batch Bayesian Optimization via Local Penalization},
	abstract = {The popularity of Bayesian optimization methods for eﬃcient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These could either be computational or physical facets of the process being optimized. Batch methods, however, require the modeling of the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. We investigate this issue and propose a highly eﬀective heuristic based on an estimate of the function’s Lipschitz constant that captures the most important aspect of this interaction—local repulsion—at negligible computational overhead. A penalized acquisition function is used to collect batches of points minimizing the non-parallelizable computational effort. The resulting algorithm compares very well, in run-time, with much more elaborate alternatives.},
	pages = {10},
	author = {Gonzalez, Javier and Dai, Zhenwen and Hennig, Philipp and Lawrence, Neil},
	langid = {english},
	file = {Gonzalez et al. - Batch Bayesian Optimization via Local Penalization.pdf:C\:\\Users\\David\\Zotero\\storage\\L7KZ9YMM\\Gonzalez et al. - Batch Bayesian Optimization via Local Penalization.pdf:application/pdf},
}

@misc{duvenaud_structure_2013,
	title = {Structure Discovery in Nonparametric Regression through Compositional Kernel Search},
	url = {http://arxiv.org/abs/1302.4922},
	doi = {10.48550/arXiv.1302.4922},
	abstract = {Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks.},
	number = {{arXiv}:1302.4922},
	publisher = {{arXiv}},
	author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
	urldate = {2022-08-08},
	date = {2013-05-13},
	eprinttype = {arxiv},
	eprint = {1302.4922 [cs, stat]},
	keywords = {Computer Science - Machine Learning, G.3, I.2.6, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\David\\Zotero\\storage\\GHB8BGKQ\\Duvenaud et al. - 2013 - Structure Discovery in Nonparametric Regression th.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\David\\Zotero\\storage\\2FM7L9WN\\1302.html:text/html},
}