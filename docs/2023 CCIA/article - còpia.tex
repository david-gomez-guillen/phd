\documentclass{IOS-Book-Article}

\usepackage{mathptmx}
\usepackage{soul}\setuldepth{article}

\usepackage[gen]{eurosym}
\usepackage{graphicx}
\usepackage{amsmath}

%\usepackage{times}
%\normalfont
%\usepackage[T1]{fontenc}
%\usepackage[mtplusscr,mtbold]{mathtime}
%
\def\hb{\hbox to 11.5 cm{}}

\begin{document}
	
	\pagestyle{headings}
	\def\thepage{}
	\begin{frontmatter}              % The preamble begins here.
		
		
		%\pretitle{Pretitle}
		\title{Bayesian optimization with additive kernels for the calibration of simulation models to perform cost-effectiveness analysis}
		
		\markboth{}{April 2023\hb}
		%\subtitle{Subtitle}
		
		\author[A,B]{\fnms{David} \snm{Gómez-Guillén}\orcid{0000-0003-1787-6482}%
			\thanks{Corresponding Author: David Gómez-Guillén, dgomez\_ext@iconcologia.net}},
		\author[B,C]{\fnms{Mireia} \snm{Díaz}\orcid{0000-0001-9360-4548}}
		\author[D]{\fnms{Josep Lluís} \snm{Arcos}\orcid{0000-0001-7751-1210}}
		and
		\author[D]{\fnms{Jesus} \snm{Cerquides}\orcid{0000-0002-3752-644X}}
		
		%\runningauthor{B.P. Manager et al.}
		\address[A]{Universitat Autònoma de Barcelona (UAB)}
		\address[B]{Institut Català d'Oncologia (ICO) - Institut d'Investigació Biomèdica de Bellvitge (IDIBELL)}
		\address[C]{Consortium for Biomedical Research in Epidemiology and Public Health - CIBERESP. Carlos III Institute of Health}
		\address[D]{Institut d'Investigació en Intel·ligència Artificial - Consell Superior d'Investigacions Científiques (IIIA-CSIC)}
		
		\begin{abstract}
			The use of mathematical simulation models of diseases in economic evaluation is an essential and common tool in medicine aimed at guiding decision-making in health. Cost-effectiveness analyses (CE) are a type of economic evaluation that assess the balance between health benefits and the economic sustainability of different health interventions. One critical aspect of these models is the accurate representation of the disease's natural history, which requires a set of parameters such as probabilities and disease burden rates. While these parameters can be obtained from scientific literature, they often need calibration to fit the model's expected outcomes. However, the calibration process can be computationally expensive and traditional optimization methods can be time-consuming due to relatively simple heuristics that may not even guarantee feasible solutions.
			In this work, we investigate the use of Bayesian optimization to enhance the calibration process by leveraging domain-specific knowledge and exploiting inherent structural properties in the solution space. Specifically, we examine the effect of additive kernel decomposition and constraint handling for efficient search.
			Our preliminary results show that this improved Bayesian optimization procedure asymptotically improves the calibration process, leading to faster convergence and better solutions for larger CE models.
		\end{abstract}
		
		\begin{keyword}
			bayesian optimization\sep gaussian processes\sep additive kernels\sep constrained optimization\sep simulation models\sep cost-effectiveness models\sep  cancer research
		\end{keyword}
	\end{frontmatter}
	\markboth{April 2023\hb}{April 2023\hb}
	%\thispagestyle{empty}
	%\pagestyle{empty}
	
	\section{Introduction}
	With the usual budgetary constraints, healthcare interventions are increasingly being evaluated based on their cost-effectiveness, ensuring equitable and efficient distribution of healthcare services. These constraints mean that not all available and effective interventions can be included in health plans. In many countries, it has become standard policy to assess the costs of new healthcare interventions in relation to their expected benefits before implementing them. Cost-effectiveness analysis (CEA) using mathematical simulation models is a crucial tool in this context, enabling us to assess the value of healthcare interventions and determine which ones offer the best value for money. By comparing the costs and benefits of alternative interventions, policymakers and healthcare providers can prioritize interventions and allocate resources to achieve the maximum health benefits for the population. Ultimately, the goal of healthcare is to improve health outcomes, and CEA plays a vital role in achieving this objective\cite{polaris}.

	CEA usually relies on simulation models that mimic disease processes to project the effects of different medical strategies on health outcomes over time. There are different types of models but some of the most common simulate as a group of individuals traverse through different health states (figure \ref{fig:lung_model}). These models can generate various outcomes, but they always produce two critical measures: the average cost and the average life expectancy, usually measured in Quality-Adjusted Life Years (QALYs).
				
	\begin{figure}[h!]
		\centering	
		\includegraphics[width=80mm]{figs/lungmodel.pdf}		
		\caption{Lung cancer markov model state diagram}	
		\label{fig:lung_model}	
	\end{figure}
		
	By comparing the incremental cost-effectiveness ratio (ICER) of two strategies (equation \ref{eq:icer}), we can determine the additional expenditure required to increase life expectancy by one year if the second strategy is adopted. This ICER can then be compared to a willingness-to-pay threshold determined by the geographical region, with strategies with ICERs falling below the threshold considered cost-effective.
	
	\begin{equation}
		\label{eq:icer}
		\textnormal{ICER}=\frac{\Delta \textrm{Cost}}{\Delta \textrm{Effectiveness}} = \frac{C_2-C_1}{E_2-E_1} \quad [\euro{}/\textrm{QALY}]
	\end{equation}
	
	In order to execute the simulations, input parameters are required to describe the disease process, such as probabilities, hazard ratios, or disease burden rates extracted from scientific literature.. Due to the inherent uncertainty of these values, it is often necessary to calibrate the model before proceeding with the analysis. Calibration consists in adjusting the input parameters until the resulting output approximates a target value identified in the scientific literature, such as disease incidence, prevalence, or mortality. This optimization process can be especially taxing for complex models, and may necessitate the use of advanced techniques to efficiently explore the solution space.
	
	Moreover, calibrations can be highly dimensional problems with many arbitrary constraints between parameters, dictated by the specific medical domain. In this work, we explore the challenges associated with calibrating CE models and propose methods to overcome them. In particular, we focus on Bayesian Optimization, a widely used technique for optimizing black-box, expensive-to-evaluate functions\cite{bayesian-opt} in a wide range of applications, including machine learning, computer vision, robotics and drug discovery, among others.
	
	\section{Background}	
	 Bayesian Optimization (BO) is a sequential model-based approach that uses a probabilistic model to iteratively encapsulate our knowledge about the target function and using it to guide the optimization process. This surrogate model starts with a prior distribution, which reflects our initial assumptions about the function, and is updated as new data is gathered. The acquisition function, which measures the promise of a new observation and directs how the search space should be explored, is maximized to identify the next point to evaluate. This process is repeated until a satisfactory solution is found, or a termination criterion is met.
	
	One popular choice for surrogate models are Gaussian Processes\cite{gaussian-processes}. These non-parametric regression models represent each observation as a random variable drawn from a Gaussian distribution $f(x) \sim \mathcal{N}(\mu(x), k(x,x))$. The mean function $\mu(x)$ and the covariance function $k(x,x')$ define the expected value of the Gaussian Process and the degree of dissimilarity between different inputs, respectively. For an assumed noise level in the data $\sigma^2$, an initial prior $\mu_0(X)$ (often $\mu_0(X)=0$), a given observation set $X=\{\vec{x_1}, ..., \vec{x_D}\}$ with labels $\vec{y}$ and a Gram matrix $K$ built from $X$ and a set of unknown observations $X_*$, a Gaussian Process $f_*$ has an analytic form for the predictive posterior distribution (equation \ref{eq:predictive_posterior}) and the marginal loglikelihood function (equation \ref{eq:loglikelihood}).
	
	\begin{equation} \label{eq:predictive_posterior}
		\begin{aligned}
			f_*|X,\vec{y},X_* & \sim \mathcal{N}(\mu(X_*), \Sigma(X_*)) \\
			\mu(X_*) & = \mu_0(X_*) + K(X_*,X)(K(X,X) + \sigma^2 I)^{-1}(\vec{y} - \mu_0(X_*)) \\
			\Sigma(X_*) & = K(X_*,X_*) + \sigma^2 I - K(X_*,X)(K(X,X) + \sigma^2 I)^{-1} K(X,X_*)
		\end{aligned}
	\end{equation}
	
	\begin{equation} \label{eq:loglikelihood}
		\begin{aligned}
			\log{p(\vec{y}|X)} &= -\frac{1}{2}\vec{y}^T (K(X,X) + \sigma^2 I)^{-1}\vec{y} - \frac{1}{2}\log{|K(X,X) + \sigma^2 I|} - \frac{n}{2}\log{2\pi}
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\begin{aligned}
			K(X,X') &= K\left(\begin{bmatrix} \vec{x_1} \\ \vdots \\ \vec{x_m} \end{bmatrix}, \begin{bmatrix} \vec{x'_1} \\ \vdots \\ \vec{x'_n} \end{bmatrix}\right) = \begin{bmatrix} 
				k(x_1,x'_1) & \dots  & k(x_1,x'_n)\\
				\vdots & \ddots & \vdots\\
				k(x_m,x'_1) & \dots  & k(x_m,x'_n)
			\end{bmatrix}
		\end{aligned}
	\end{equation}
	
	The covariance function or kernel is the mechanism to give a Gaussian Process its expressive power, and its choice will heavily depend on the kind of function we aim to model\cite{kernel-composition}. The squared exponential (SE) kernel $k(x,x') = \sigma^2 e^{-\frac{||x-x'||^2}{2l^2}}$ is a popular choice, despite significant drawbacks such as its locality and sensitivity to the curse of dimensionality\cite{curse-dimensionality}. As a result, the SE kernel can be an inefficient option in many high-dimensional Bayesian Optimization problems.
	
	Many techniques are under active research to address high-dimensional problems with Gaussian Processes\cite{gp-high-dim}\cite{gp-high-dim2}. One of these techniques is additive kernel decomposition\cite{gp-additive}. The full additive kernel is shown in equation \ref{eq:additive1} as the weighted sum of the additive kernels of all order interactions (equation \ref{eq:additive2}), when a SE kernel is used as the base kernel (equation \ref{eq:additive3}).
	
	\begin{equation} \label{eq:additive1}
		\begin{aligned}
			k_{add}(x,x') &= \sum_{i=1}^D{\sigma_i^2 k_{add_i}(x_i,x_i')}
		\end{aligned}
	\end{equation}
	
	\begin{equation} \label{eq:additive2}
		\begin{aligned}
			k_{add_j}(x,x') &= \sum_{1\leq i_1 < i_2 < \ldots < i_j\leq D} \left[\prod_{d=1}^{j} k_{i_d}(x_{i_d},x_{i_d}') \right]
		\end{aligned}
	\end{equation}
	
	\begin{equation} \label{eq:additive3}
		\begin{aligned}
			k_i(x,x') &= e^{\frac{(x_i-x_i')^2}{2l_i^2}}
		\end{aligned}
	\end{equation}
	
	Additive kernels suffer from the non-identifiability problem for summed functions. To ensure a unique decomposition, Lu et al\cite{gp-additive-orthogonal} proposed an extension of additive kernels by including an extra constant kernel with an additional variance hyperparameter $\sigma_0^2$ and an orthogonality constraint to generate Orthogonal Additive Kernels (OAK)\cite{gp-additive-orthogonal}. Assuming a normal input distribution $x_i \sim \mathcal{N}(\mu_i, \delta_i^2)$, the following constrained base kernel is derived:
	
	\begin{equation} \label{eq:additive-orthogonal}
		\begin{aligned}
			k_{add_{OAK}}(x,x') &= \sum_{i=0}^D{\sigma_i^2  \tilde{k}_{add_i}(x_i,x_i')} \\
			\tilde{k}_{add_0}(x,x') &= 1\\
			\tilde{k}_{add_j}(x,x') &= \sum_{1\leq i_1 < i_2 < \ldots < i_j\leq D} \left[\prod_{d=1}^{j} \tilde{k}_{i_d}(x_{i_d},x_{i_d}') \right]\\		
			\tilde{k}_i(x,x') &= e^{\frac{(x_i-x_i')^2}{2l_i^2}} - \frac{l_i\sqrt{l_i^2 + 2\delta_i^2}}{l_i^2 + \delta_i^2} e^{-\frac{(x_i-\mu_i)^2 + (x_i'-\mu_i)^2}{2(l_i^2 + \delta_i^2)}}
		\end{aligned}
	\end{equation}
	
	One important advantage of these additive kernels is that we can interpret the $\sigma_i^2$ as the contribution of each individual order to the total kernel. Since many problems often rely on a few low-order interactions, we can truncate the rest of higher orders and limit the computational cost while retaining most of the information present in the full decomposition. To achieve this, OAK kernels can be useful in accurately identifying each contribution and providing an accurate representation on the actual composition on the function.
	
	\section{Methodology}
	\subsection{Description of the Simulation Model}
	We use a lung cancer model used in a published cost-effectiveness analysis\cite{lung-model} as a fast benchmark for Bayesian Optimization on simulation models for cost-effectiveness analysis. This Markov-based microsimulation model simulates a cohort's progression through seven different health states from 35 to 79 years of age, in monthly intervals. The transition probabilities used in the model were age-specific, with distinct values for each 5-year age group (i.e. 35-39, 40-44, ..., 75-79). The state diagram for this model is pictured in figure \ref{fig:lung_model}.
		
	To simplify the calibration tests, certain details were ignored in the original model, such as gender differentiation, smoking prevalence, and quitting probabilities for smokers. Additionally, the survival state was removed, resulting in only six states being considered and 6x6 transition matrices being used. Certain inherent constraints, such as ensuring that the sum of the probabilities in each row equals one or that certain probabilities are zero, were imposed on the matrices. This allowed the number of parameters to be optimized per matrix to be reduced from 36 to 11. It is worth noting that despite there being 9 age groups, each with its own set of parameters, only the first age group was calibrated in this study. As a result, the problem was simplified to only 11 parameters, rather than the original $11\cdot 9=99$ parameters associated with the full simulation.
	
	The calibration target for the model was defined as the weighted sum of the euclidean distances between the observed and expected outputs of interest, namely lung cancer incidence (45\%), lung cancer mortality (45\%) and mortality from other causes (10\%), computed for each age group.
	
	\subsection{Optimization Methods}
	In cost-effectiveness modeling, it is common to have an initial estimate of a good solution based on approximate values found in the scientific literature. For all optimization experiments conducted, a solution space of plus or minus $\pm 50\%$ was considered for each input variable, centered around this initial value.
	
	First, we used different optimization methods to illustrate the performance differences between regular Bayesian Optimization and classical methods. For this purpose we used python implementations of commonly used methods: a hill-climbing technique (Nelder-Mead\footnote{\url{https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html}}\cite{nelder-mead}), metaheuristics (Simulated Annealing (SA)\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.dual_annealing.html}}\cite{simulated-annealing} and Particle Swarm Optimization\footnote{\url{https://pyswarms.readthedocs.io/en/latest/}}\cite{pso}), and Bayesian Optimization with Gaussian Processes\footnote{\url{https://secondmind-labs.github.io/trieste/1.1.2/index.html}}. The default hyperparameter values were used for these methods, except for Particle Swarm Optimization, where the number of particles was set to 1,000 times the number of age groups calibrated.
	
	We also developed a new BO implementation with Gaussian Processes from scratch using the R programming language. This implementation was used as a rapid prototyping environment to evaluate different enhancements to the optimization process for our specific domain, without being concerned by execution time at this stage. This implementation uses the Expected Improvement acquisition function, with Particle Swarm Optimization to search for its maximum. Finally, both the SE and the OAK kernels were implemented. Runtime optimizations such as GPU use are beyond the scope of this work.
	
	Before starting the BO procedure we learn the lengthscales $l_1, ..., l_D$ and the variances $\sigma_0^1, \sigma_1^2, ..., \sigma_n^2$ of the OAK kernel in a two stage process. In the first stage, we maximize the marginal likelihood for each lengthscale separately. In the second stage, we maximize the marginal likelihood for the whole set of variances. This approach allows us to break down a complex task for high-dimensional problems into low-dimensional, manageable problems. This is particularly important when working with larger models.
	
	\section{Results}
	The execution times of the minimal version of the lung cancer model presented above were unsurprising. The regular trieste implementation of BO using Gaussian Processes with SE kernels takes significantly more time than the alternatives, as seen in table \ref{tab:result-methods}.
		
	\begin{table}[h!]
		\centering
		%\begin{center}
		\begin{tabular}{llll} 
			\hline
			Method & Error & Time (s) & Evaluations \\ 
			\hline
			Nelder-Mead & 0.16857 & 0.95 & 839 \\ 
			Simulated Annealing & 0.11296 & 21.24 & 22709 \\ 
			Particle Swarm & 0.11271 & 5.1141 & 5177 \\
			Bayesian Optimization & 0.11718 & 689.38 & 130 \\
			\hline
		\end{tabular}
		\caption{Comparison of different optimization methods in the lung cancer model with one age group.}
		\label{tab:result-methods}
		%\end{center}
	\end{table}
	
	The model tested was designed to be computationally inexpensive, taking less than 10ms to evaluate. By introducing arbitrary delays in the model we can observe the relationship of optimization times and model simulation times for different optimization methods in figure \ref{fig:sim_times}. To account for parallelism in the Particle Swarm method, we also plotted a theoretical implementation with perfect parallelization using 8 cores, assuming eight-times faster calibration times. For very fast models the inference overhead of BO dominates and other methods are able to calibrate faster by simulating the model many times. However, as the simulation time increases, the Bayesian method efficient approach in number of function evaluations results in faster calibration times. Specifically, for a model with 11 parameters and simulation times of less than 250ms we observed the Bayesian approach outperform the alternative methods.
	
	In contrast, as the dimensionality of our problem grows, the bayesian method overhead increased significantly, as shown in the y-intercept of figure \ref{fig:sim_times}. The calibration times for the other methods also increased but, overall, the simulation time threshold where the Bayesian approach outperforms the other methods increases exponentially with the number of parameters, from 0.2 seconds to 0.35, 0.95 and 3.25 seconds. The bottom left plot in figure \ref{fig:sim_times} projects that Bayesian calibration of all 99 parameters would achieve the best performance when each simulation takes approximately 5 minutes of computation.
	
	\begin{figure}[h!]
		\centering	
		\includegraphics[width=\textwidth]{figs/crit\_times\_log.pdf}		
		\caption{Total calibration time in logarithmic scale against model simulation time required to attain similar levels of error. The bottom left figure shows the exponential trend (in log scale) in the necessary simulation time before the bayesian method becomes the fastest method, as a function of the number of parameters. The bottom right figure is a zoomed-in plot of the same figure in linear scale.}
		\label{fig:sim_times}	
	\end{figure}
	
	In any case, the focus of our research in this work is the number of evaluations, where we see a sharp drop in error when using BO to achieve a similar level of accuracy compared to other methods, as shown in figure \ref{fig:method_comparison}. Although each iteration requires a significant amount of time due to the bayesian inference step, this overhead will become less relevant as the size of the model increases.
	
	\begin{figure}[h!]
		\centering	
		\includegraphics[width=\textwidth]{figs/methods\_n1.pdf}		
		\caption{Time series of the lung cancer model calibration error (using 1 age group). The error is plotted against the number of evaluations by method.}
		\label{fig:method_comparison}	
	\end{figure}
	
	While exploring the results of BO with OAK kernels we found that one of the variables had very significant explanatory power by itself, which could produce misleading results in the comparison. To address this issue, we introduced a third univariate SE kernel that considers only this variable. Figure \ref{fig:results_oak} shows the average progression of the error during the optimization process for the three kernels and their confidence interval for a sample of 30 random executions. The univariate SE kernel shows a lower average error and lower deviation than the full SE kernel, due to the reduction in dimensionality of the problem that allows for an easier exploration of the solution space, with barely any information loss. However, the OAK kernel under a normality assumption for the inputs is able to efficiently search the full 11-dimensional space to reach even better average results than the univariate SE kernel, while reducing the deviation as the optimization progresses.
	
	\begin{figure}[h!]
		\centering	
		\includegraphics[width=\textwidth]{figs/results.pdf}		
		\caption{Time series of the BO error, where lines represent the average and the shaded area the confidence intervals. We used three different kernels: the SE kernel (blue), the univariate SE kernel using only the most significant variable (green) and the OAK kernel under a normality assumption for the inputs (red).}
		\label{fig:results_oak}	
	\end{figure}
	
	\section{Discussion}
	BO is currently considered a state-of-the-art optimization method in various domains that involve costly functions to evaluate. When the computational cost of the target function is large enough, Bayesian Optimization might be the natural choice. But when the function is not as expensive, there are two other critical points in each BO iteration that must be taken into account: the surrogate model regression and the acquisition function optimization.
	
	Regarding the former, SE kernels suffer greatly from the curse of dimensionality. In high-dimensional problems, the number of observations to explore the solution space quickly increases. This renders the regression unfeasible when trying to invert large Gram matrices to calculate the posterior predictive distribution. To address this issue, OAK kernels can reduce the number of necessary observations, mitigate the effects of an expanding Gram matrix and enhance the efficiency of the search. The observed scaling issues in figure \ref{fig:sim_times}, resulting from increasing dimensionality, justify the need for high-dimensional improvements such as OAK kernels. Nevertheless, our experiments showed that the asymptotic behaviour of Bayesian Optimization persisted, making it the optimal choice for sufficiently large models.
	
	The optimization of the acquisition function is the initial bottleneck, where the number of observations is still small enough so that the surrogate model regression is not yet a problem. As the search space remains constant, this acquisition optimization doesn't become much more expensive as more data is observed. We used Particle Swarm Optimization as an easy way to take advantage of parallelism in this area, but other approaches mentioned in the next section are being considered as well\cite{acquisition-functions}.
	
	Lu et al\cite{gp-additive-orthogonal} mention that an interesting direction of work would be to extend OAK kernels to Bayesian Optimization leveraging the inferred low-order representation. In our tests we show that, even with a straightforward application of OAK kernels on this simple example, a slight improvement over the SE kernel is noticeable. This improvement is expected to be more meaningful for complex models, where more structure can be leveraged. It is interesting to note that these results hold even though some assumptions of the model were not met. Specifically, hyperparameter tuning was performed with a dataset sampled from a uniform input distribution, while the constrained kernels were calculated assuming normality in the input. Even if these distributions were consistent, we still would have the problem of determining the input distribution for the actual optimization process, which would be neither normal nor uniform.
	
	%gp max likelihood ill-posed -> impose gaussian noise
	
	\section{Future Work and Conclusions}
	One particular aspect that we did not incorporate into this article is constraint handling. Simulation models can be highly constrained problems, and these constraints are another expression of the structure of the solution space. We have been able to manage arbitrary constraints successfully using additional surrogate models and a new Constrained Expected Improvement acquisition function, as introduced by Gardnet et al\cite{gp-constraints}.
	
	We also mentioned in the discussion the convenience of exploiting the parallelization potential of the different areas of the optimization process. For that purpose we use the Particle Swarm method for the optimization of the acquisition function, but other more sophisticated venues for parallelization include batched optimization\cite{gp-batch}\cite{gp-batch2}, parallel acquisition functions\cite{gp-parallel-acq-func} or GPU approaches\cite{gp-gpu} among others.
	
	Our research group recognizes the importance of efficiently calibrating increasingly complex models, injecting relevant domain knowledge in the process. The content of this work is only the beginning of several enhancements that are being implemented to have the tools to work with more challenging models in the future.
	
	\section{Acknowledgement}
	This work was supported by a grant from the Instituto de Salud Carlos III-ISCIII (Spanish Government) co-funded by European Regional Development Fund, a way to build Europe (CIBERESP CB06/02/0073, PI19/01118), also with the support of the Secretariat for Universities and Research of the Department of Business and Knowledge of the Government of Catalonia. Grants to support the activities of research groups (SGR 2017–2021). Grant number 2021SGR1029. We thank the CERCA Programme and Generalitat de Catalunya for institutional support.
	
	
	
	\begin{thebibliography}{99}
		
		\bibitem{polaris}
		Cost-Effectiveness Analysis | POLARIS | Policy and Strategy | CDC (Centers for Disease Control and Prevention). 2021. Available from: https://www.cdc.gov/policy/polaris/economics/cost-effectiveness/index.html
				
		\bibitem{ce-models}
		Kuntz KM, Russell LB, Owens DK, Sanders GD, Trikalinos TA, Salomon JA. Decision Models in Cost-Effectiveness Analysis. In: Neumann PJ, Ganiats TG, Russell LB, Sanders GD, Siegel JE, editors. Cost-Effectiveness in Health and Medicine. Oxford University Press; 2016. p. 0. doi: 10.1093/acprof:oso/9780190492939.003.0005				
		\bibitem{ce-calibration}
		Moriña D, Díaz M. Calibration Approach Impact on Health and Cost-Effectiveness Outcomes in a Decision Analytic Framework. Value in Health. 2017 Oct 1;20(9):A407–8. doi: 10.1016/j.jval.2017.08.059
		
		\bibitem{bayesian-opt}
		Bergstra J, Bardenet R, Bengio Y, Kégl B. Algorithms for Hyper-Parameter Optimization. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2011.
		
		\bibitem{gaussian-processes}
		Rasmussen CE, Williams CKI. Gaussian processes for machine learning. Cambridge, Mass: MIT Press; 2006. 248 p. (Adaptive computation and machine learning). doi: 10.7551/mitpress/3206.001.0001
		
		\bibitem{kernel-composition}
		Duvenaud D, Lloyd J, Grosse R, Tenenbaum J, Zoubin G. Structure Discovery in Nonparametric Regression through Compositional Kernel Search. In: Proceedings of the 30th International Conference on Machine Learning. PMLR; 2013. p. 1166–74.
			
		\bibitem{curse-dimensionality}
		Bengio Y. On the challenge of learning complex functions. Prog Brain Res. 2007;165:521–34. doi: 10.1016/S0079-6123(06)65033-4 
		
		\bibitem{gp-additive}
		Duvenaud DK, Nickisch H, Rasmussen C. Additive Gaussian Processes. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2011.
		
		\bibitem{acquisition-functions}
		Wilson JT, Hutter F, Deisenroth MP. Maximizing acquisition functions for Bayesian optimization. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems. Red Hook, NY, USA: Curran Associates Inc.; 2018. p. 9906–17. (NIPS’18). 
		
		\bibitem{gp-additive-orthogonal}
		Lu X, Boukouvalas A, Hensman J. Additive Gaussian Processes Revisited. In: Proceedings of the 39th International Conference on Machine Learning [Internet]. PMLR; 2022 [cited 2022 Oct 5]. p. 14358–83.
		
		\bibitem{gp-high-dim}
		Durrande N, Ginsbourger D, Roustant O. Additive covariance kernels for high-dimensional Gaussian process modeling. Annales de la Faculté de Sciences de Toulouse. 2012;Tome 21(numéro 3):481. 
		
		\bibitem{gp-high-dim2}
		Binois M, Wycoff N. A Survey on High-dimensional Gaussian Process Modeling with Application to Bayesian Optimization. ACM Trans Evol Learn Optim. 2022 Aug 16;2(2):8:1-8:26. doi: 10.1145/3545611
		
		\bibitem{lung-model}
		Diaz M, Garcia M, Vidal C, Santiago A, Gnutti G, Gómez D, Trapero-Bertran M, Fu M; Lung Cancer Prevention LUCAPREV research group. Health and economic impact at a population level of both primary and secondary preventive lung cancer interventions: A model-based cost-effectiveness analysis. Lung Cancer. 2021 Sep;159:153-161. doi: 10.1016/j.lungcan.2021.06.027
		
		\bibitem{nelder-mead}
		Nelder JA, Mead R. A Simplex Method for Function Minimization. The Computer Journal. 1965 Jan 1;7(4):308–13. doi: 10.1093/comjnl/7.4.308
		
		\bibitem{simulated-annealing}
		Kirkpatrick S, Gelatt CD, Vecchi MP. Optimization by Simulated Annealing. Science. 1983 May 13;220(4598):671–80. doi: 10.1126/science.220.4598.671
		
		\bibitem{pso}
		Bonyadi MR, Michalewicz Z. Particle Swarm Optimization for Single Objective Continuous Space Problems: A Review. Evolutionary Computation. 2017 Mar;25(1):1–54. doi: 10.1162/EVCO\_r\_00180
		
		\bibitem{gp-constraints}
		Gardner JR, Kusner MJ, Xu Z, Weinberger KQ, Cunningham JP. Bayesian optimization with inequality constraints. In: Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32. Beijing, China: JMLR.org; 2014. p. II-937-II–945. (ICML’14). 
		
		\bibitem{gp-batch}
		González J, Dai Z, Hennig P, Lawrence N. Batch Bayesian Optimization via Local Penalization. In: Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) [Internet]. 2016. p. 648–57. (JMLR Workshop and Conference Proceedings; vol. 51).
		
		\bibitem{gp-batch2}
		Rontsis N, Osborne MA, Goulart PJ. Distributionally Ambiguous Optimization for Batch Bayesian Optimization. Journal of Machine Learning Research. 2020;21(149):1–26. doi: 10.5555/3455716.3455865
		
		\bibitem{gp-parallel-acq-func}
		Wang J, Clark SC, Liu E, Frazier PI. Parallel Bayesian Global Optimization of Expensive Functions. Operations Research. 2020;68(6):1850–65. doi: 10.1287/opre.2019.1966
		
		\bibitem{gp-gpu}
		Wang K, Pleiss G, Gardner J, Tyree S, Weinberger KQ, Wilson AG. Exact Gaussian Processes on a Million Data Points. In: Advances in Neural Information Processing Systems. Curran Associates, Inc.; 2019.

		
	\end{thebibliography}
	
\end{document}
